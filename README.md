# Amazon Fashion Review NLP Pipeline (BERT + GPT-2)

Amazon Fashion ë¦¬ë·° ë°ì´í„°ì…‹ì„ ê¸°ë°˜ìœ¼ë¡œ ë¦¬ë·° ë¬¸ì¥ì„ **ì¹´í…Œê³ ë¦¬(ë°°ì†¡/ì‚¬ì´ì¦ˆ/ìƒ‰ìƒ/í€„ë¦¬í‹°)** ë° **ê°ì„±(ê¸ì •/ë¶€ì •)** ê¸°ì¤€ìœ¼ë¡œ ë¶„ë¥˜í•˜ê³ ,  
HuggingFace Transformers ê¸°ë°˜ GPT-2 Fine-tuningì„ í†µí•´ **ì¡°ê±´ë¶€ ë¦¬ë·° í…ìŠ¤íŠ¸ ìƒì„± ëª¨ë¸**ì„ êµ¬ì¶•í•œ í”„ë¡œì íŠ¸ì…ë‹ˆë‹¤.

ë³¸ í”„ë¡œì íŠ¸ëŠ” ë°ì´í„° ì „ì²˜ë¦¬ â†’ ìë™ ë¼ë²¨ë§(BERT) â†’ í•™ìŠµ ë°ì´í„°ì…‹ êµ¬ì¶• â†’ ìƒì„± ëª¨ë¸ í•™ìŠµ(GPT-2) â†’ ìƒì„± ê²°ê³¼ ê²€ì¦ê¹Œì§€  
NLP íŒŒì´í”„ë¼ì¸ì„ ì§ì ‘ ì„¤ê³„í•˜ê³  êµ¬í˜„í•˜ëŠ” ê²ƒì„ ëª©í‘œë¡œ ì§„í–‰í–ˆìŠµë‹ˆë‹¤.

---

## ğŸ”¥ Project Overview

- **Dataset**: Amazon Fashion Review Dataset (UCSD Amazon Review Data)
- **Goal**: ë¦¬ë·° í…ìŠ¤íŠ¸ë¥¼ êµ¬ì¡°í™”(ì¹´í…Œê³ ë¦¬/ê°ì„±)í•˜ê³ , ì¡°ê±´ì— ë§ëŠ” ë¦¬ë·° ë¬¸ì¥ì„ ìƒì„±í•˜ëŠ” ëª¨ë¸ êµ¬ì¶•
- **Output**: ì¹´í…Œê³ ë¦¬/ê°ì„±ë³„ ë¦¬ë·° ìƒì„± ê²°ê³¼ ë¹„êµ ë° ì‹¤í—˜ ìˆ˜í–‰

---

## ğŸ§© Pipeline

ë³¸ í”„ë¡œì íŠ¸ëŠ” ì•„ë˜ì™€ ê°™ì€ íë¦„ìœ¼ë¡œ ì§„í–‰ë˜ì—ˆìŠµë‹ˆë‹¤.

1. **Raw ë¦¬ë·° ë°ì´í„° ìˆ˜ì§‘ ë° EDA**
2. **í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ ë° ë¬¸ì¥ ë‹¨ìœ„ ë¶„ë¦¬**
3. **BERT ê¸°ë°˜ ë¶„ë¥˜ ëª¨ë¸ í•™ìŠµ**
4. **ì „ì²´ ë¦¬ë·° ë°ì´í„° ìë™ ë¼ë²¨ë§ (Category + Sentiment)**
5. **ì¹´í…Œê³ ë¦¬/ê°ì„±ë³„ í•™ìŠµ ë°ì´í„°ì…‹ êµ¬ì¶• (ì´ 8ê°œ txt/csv)**
6. **GPT-2 Fine-tuningì„ í†µí•œ ì¡°ê±´ë¶€ ë¦¬ë·° ìƒì„± ëª¨ë¸ í•™ìŠµ**
7. **Top-k / Top-p Sampling ê¸°ë°˜ ìƒì„± ê²°ê³¼ ë¹„êµ ì‹¤í—˜**

---

## ğŸ“Š Dataset

### Raw Data
- Amazon Fashion ë¦¬ë·° ë°ì´í„° ì•½ **165,000ê±´**
- ì£¼ìš” ì»¬ëŸ¼:
  - overall (ë³„ì )
  - verified (êµ¬ë§¤ ì¸ì¦ ì—¬ë¶€)
  - year
  - reviewText

### Labeling Strategy
- ë¦¬ë·° ë¬¸ì¥ì„ ì•„ë˜ 4ê°œ ì¹´í…Œê³ ë¦¬ë¡œ ë¶„ë¥˜
  - ë°°ì†¡(Delivery)
  - ì‚¬ì´ì¦ˆ(Size)
  - ìƒ‰ìƒ(Color)
  - í€„ë¦¬í‹°(Quality)

- ê°ì„±(Sentiment)ì€ ê¸ì •/ë¶€ì • ê¸°ì¤€ìœ¼ë¡œ ë¶„ë¥˜
  - Positive
  - Negative

ìµœì¢…ì ìœ¼ë¡œ ì¹´í…Œê³ ë¦¬ Ã— ê°ì„± ê¸°ì¤€ìœ¼ë¡œ ì´ **8ê°œ ë°ì´í„°ì…‹**ì„ êµ¬ì¶•í–ˆìŠµë‹ˆë‹¤.

---

## ğŸ§ª Preprocessing

- ì •ê·œí‘œí˜„ì‹ ê¸°ë°˜ í…ìŠ¤íŠ¸ ì •ì œ (íŠ¹ìˆ˜ë¬¸ì/ë¶ˆí•„ìš” íŒ¨í„´ ì œê±°)
- ë¬¸ì¥ ë‹¨ìœ„ ë¶„ë¦¬ í›„ í•™ìŠµ ê°€ëŠ¥í•œ í˜•íƒœë¡œ ë³€í™˜
- ì¹´í…Œê³ ë¦¬/ê°ì„± ë¼ë²¨ ê¸°ì¤€ìœ¼ë¡œ ë°ì´í„°ì…‹ ë¶„í•  ë° txt íŒŒì¼ ìƒì„±

---

## ğŸ¤– Models

### 1) BERT ê¸°ë°˜ ë¬¸ì¥ ë¶„ë¥˜ ëª¨ë¸
- ëª©ì : ì „ì²´ ë¦¬ë·° ë°ì´í„°ì— ëŒ€í•´ ì¹´í…Œê³ ë¦¬/ê°ì„± ë¼ë²¨ì„ ìë™ ë¶€ì—¬í•˜ê¸° ìœ„í•œ ë¶„ë¥˜ ëª¨ë¸ êµ¬ì¶•
- ê²°ê³¼: ìë™ ë¼ë²¨ë§ ê¸°ë°˜ ë°ì´í„°ì…‹ êµ¬ì¶•ì— í™œìš©

### 2) GPT-2 Fine-tuning ê¸°ë°˜ ë¦¬ë·° ìƒì„± ëª¨ë¸
- HuggingFace Transformers ê¸°ë°˜ GPT-2 ëª¨ë¸ì„ Fine-tuning
- Trainer APIë¥¼ í™œìš©í•˜ì—¬ í•™ìŠµ ìˆ˜í–‰
- ì¹´í…Œê³ ë¦¬/ê°ì„±ë³„ í…ìŠ¤íŠ¸ ìƒì„± ê²°ê³¼ë¥¼ ë¹„êµ ì‹¤í—˜

---

## âœ¨ Text Generation

í•™ìŠµëœ GPT-2 ëª¨ë¸ì„ ê¸°ë°˜ìœ¼ë¡œ ë‹¤ìŒê³¼ ê°™ì€ ì„¤ì •ì„ ì‚¬ìš©í•´ ë¦¬ë·° í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í–ˆìŠµë‹ˆë‹¤.

- Sampling Strategy:
  - **Top-k sampling**
  - **Top-p sampling (nucleus sampling)**

ìƒì„±ëœ ê²°ê³¼ë¥¼ ìƒ˜í”Œë§í•˜ì—¬ ì¹´í…Œê³ ë¦¬ë³„ ë¬¸ì¥ ìƒì„± í’ˆì§ˆì„ ë¹„êµí–ˆìŠµë‹ˆë‹¤.

---

## ğŸ“Œ Example Outputs

Below are sample generated sentences from the fine-tuned GPT-2 model:

### Delivery (Positive)
- "The package arrived earlier than expected and was well packed."

### Delivery (Negative)
- "The delivery was delayed and the packaging was damaged."

### Size (Positive)
- "Fits perfectly and feels comfortable to wear."

### Size (Negative)
- "The size was much smaller than expected."

*(The generated sentences are sampled results and may vary depending on sampling parameters.)*

---

## ğŸ›  Tech Stack

- Python
- Pandas / NumPy
- PyTorch
- HuggingFace Transformers
- BERT
- GPT-2
- Google Colab

---

## ğŸ“‚ Project Structure

TextMining_TeamProject/
â”œâ”€â”€ data/
â”‚ â”œâ”€â”€ raw/
â”‚ â”œâ”€â”€ processed/
â”‚ â”œâ”€â”€ labeled/
â”‚ â””â”€â”€ txt_dataset/
â”œâ”€â”€ notebooks/
â”‚ â”œâ”€â”€ preprocessing.ipynb
â”‚ â”œâ”€â”€ bert_training.ipynb
â”‚ â”œâ”€â”€ gpt2_finetuning.ipynb
â”‚ â””â”€â”€ text_generation.ipynb
â”œâ”€â”€ outputs/
â”‚ â””â”€â”€ generated_samples.txt
â””â”€â”€ README.md


*(structure may differ depending on local environment)*

---

## ğŸš€ How to Run

### 1. Install Dependencies
```bash
pip install transformers accelerate torch pandas numpy
```

2. Run Notebook Pipeline
Recommended execution order:

preprocessing.ipynb

bert_training.ipynb

gpt2_finetuning.ipynb

text_generation.ipynb

ğŸ“Œ Key Takeaways
í…ìŠ¤íŠ¸ ë°ì´í„° ì „ì²˜ë¦¬ ë° êµ¬ì¡°í™” ê²½í—˜

BERT ê¸°ë°˜ ë¶„ë¥˜ ëª¨ë¸ í•™ìŠµ ë° ìë™ ë¼ë²¨ë§ ìˆ˜í–‰

HuggingFace Trainer ê¸°ë°˜ GPT-2 Fine-tuning ê²½í—˜

Top-k / Top-p Sampling ê¸°ë°˜ ìƒì„± ê²°ê³¼ ë¹„êµ ì‹¤í—˜ ìˆ˜í–‰

NLP ëª¨ë¸ í•™ìŠµ íŒŒì´í”„ë¼ì¸ì„ ì§ì ‘ ì„¤ê³„í•˜ë©° End-to-End íë¦„ì„ ê²½í—˜

ğŸ”— Data Source
Amazon Review Data (UCSD)
https://jmcauley.ucsd.edu/data/amazon/

ğŸ“Œ Notes
ë³¸ í”„ë¡œì íŠ¸ëŠ” í•™ë¶€ í…ìŠ¤íŠ¸ë§ˆì´ë‹ ìˆ˜ì—… íŒ€ í”„ë¡œì íŠ¸ë¡œ ì§„í–‰ë˜ì—ˆìœ¼ë©°,
í•™ìŠµ ë° ì‹¤í—˜ ëª©ì ì˜ ì—°êµ¬/ì‹¤ìŠµ ê¸°ë°˜ í”„ë¡œì íŠ¸ì…ë‹ˆë‹¤.
